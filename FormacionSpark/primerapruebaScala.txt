Microsoft Windows [VersiÃ³n 10.0.19044.1706]
(c) Microsoft Corporation. Todos los derechos reservados.

C:\Users\antoniodavid.moreno>spark-shell
22/06/08 12:26:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/06/08 12:26:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Spark context Web UI available at http://L2204025.mshome.net:4041
Spark context available as 'sc' (master = local[*], app id = local-1654683991977).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.3
      /_/

Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_331)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 22/06/08 12:26:43 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
scala
<console>:24: error: package scala is not a value
       scala
       ^

scala> spark.version
res1: String = 3.0.3

scala> val strings = spark.read.text("C:/Spark3/README.md")
strings: org.apache.spark.sql.DataFrame = [value: string]

scala> strings.show(10,false)
+--------------------------------------------------------------------------------+
|value                                                                           |
+--------------------------------------------------------------------------------+
|# Apache Spark                                                                  |
|                                                                                |
|Spark is a unified analytics engine for large-scale data processing. It provides|
|high-level APIs in Scala, Java, Python, and R, and an optimized engine that     |
|supports general computation graphs for data analysis. It also supports a       |
|rich set of higher-level tools including Spark SQL for SQL and DataFrames,      |
|MLlib for machine learning, GraphX for graph processing,                        |
|and Structured Streaming for stream processing.                                 |
|                                                                                |
|<https://spark.apache.org/>                                                     |
+--------------------------------------------------------------------------------+
only showing top 10 rows

scala> strings.count()
res4: Long = 108

-------------------------------

scala> val strings = spark.read.text("C:/Spark3/README.md")
strings: org.apache.spark.sql.DataFrame = [value: string]
                           
scala> val filtered = strings.filter($"value".contains("Spark"))
filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: string]

scala> filtered.count()
res0: Long = 19

scala>

Salgo con Ctrl+D