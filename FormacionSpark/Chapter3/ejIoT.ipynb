{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b48bfb-c067-48ca-b16d-1cb761d471bb",
   "metadata": {},
   "source": [
    "# Preguntas Internet of Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f823fb5f-85b3-4378-8aa7-caa01fc4854b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://5fcc63a3e7cf:4042\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1655396568314)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@f54ec07\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5818eab5-089f-492e-b1d3-31ce40e7e85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a1d15c-9ba3-4750-aaea-e48d5b7243bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@526376ab\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a4f6e-e468-40d1-9f50-6ad3697a991a",
   "metadata": {},
   "source": [
    "## CASE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcaf504c-f3e8-48ac-9cb1-23d652a590dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceIoTData\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DeviceIoTData (battery_level: Long, c02_level: Long,\n",
    "cca2: String, cca3: String, cn: String, device_id: Long,\n",
    "device_name: String, humidity: Long, ip: String, latitude: Double,\n",
    "lcd: String, longitude: Double, scale:String, temp: Long,\n",
    "timestamp: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a22750-44a0-4467-84d8-574488dc87d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.read\n",
    "    .json(\"iot_devices.json\")\n",
    "    .as[DeviceIoTData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bd9fe4-3525-4174-980a-fe45caee1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+-------------+--------+-----+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|           cn|device_id|         device_name|humidity|           ip|latitude|  lcd|longitude|  scale|temp|    timestamp|\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+-------------+--------+-----+---------+-------+----+-------------+\n",
      "|            8|      868|  US| USA|United States|        1|meter-gauge-1xbYRYcj|      51| 68.161.225.1|    38.0|green|    -97.0|Celsius|  34|1458444054093|\n",
      "|            7|     1473|  NO| NOR|       Norway|        2|   sensor-pad-2n2Pea|      70|213.161.254.1|   62.47|  red|     6.15|Celsius|  11|1458444054119|\n",
      "|            2|     1556|  IT| ITA|        Italy|        3| device-mac-36TWSKiT|      44|    88.36.5.1|   42.83|  red|    12.83|Celsius|  19|1458444054120|\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+-------------+--------+-----+---------+-------+----+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fc7e579-9b44-4b63-8d17-3775622e994a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterTempDS: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterTempDS = ds.filter(d => {d.temp > 30 && d.humidity > 70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ee4ee8-b0b4-42af-b345-6bd982dd0c2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 9) (5fcc63a3e7cf executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @3a7bd559; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @19c32eb7)",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 9) (5fcc63a3e7cf executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @3a7bd559; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @19c32eb7)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2728)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2935)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:326)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:808)",
      "  ... 40 elided",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @3a7bd559; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @19c32eb7)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "filterTempDS.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ce3cbe-bca7-4a12-860d-5178f3427119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+-----+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|           cn|device_id|         device_name|humidity|             ip|latitude|  lcd|longitude|  scale|temp|    timestamp|\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+-----+---------+-------+----+-------------+\n",
      "|            0|     1466|  US| USA|United States|       17|meter-gauge-17zb8...|      98|161.188.212.254|   39.95|  red|   -75.16|Celsius|  31|1458444054129|\n",
      "|            9|      986|  FR| FRA|       France|       48|  sensor-pad-48jt4eL|      97|    90.37.208.1|   43.88|green|      4.9|Celsius|  31|1458444054151|\n",
      "|            8|     1436|  US| USA|United States|       54|sensor-pad-5410CW...|      73|  204.15.64.249|   32.89|  red|  -117.13|Celsius|  34|1458444054155|\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+-----+---------+-------+----+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.select(\"*\")\n",
    "    .where($\"temp\">30 && $\"humidity\">70)\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9771bae3-462f-4f54-a632-026be6d244e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------+----+\n",
      "|temp|         device_name|device_id|cca3|\n",
      "+----+--------------------+---------+----+\n",
      "|  34|meter-gauge-1xbYRYcj|        1| USA|\n",
      "|  28|   sensor-pad-4mzWkz|        4| USA|\n",
      "|  27|sensor-pad-6al7RT...|        6| USA|\n",
      "|  27|sensor-pad-8xUD6p...|        8| JPN|\n",
      "|  26|sensor-pad-10Bsyw...|       10| USA|\n",
      "+----+--------------------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.select(\"temp\",\"device_name\",\"device_id\",\"cca3\")\n",
    "    .where($\"temp\">25)\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f332ea-f01f-4be0-bcf7-cbfc5f2e61e3",
   "metadata": {},
   "source": [
    "## Pregunta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e1de7b-2416-4df2-b3aa-6d95a6190793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- battery_level: long (nullable = true)\n",
      " |-- c02_level: long (nullable = true)\n",
      " |-- cca2: string (nullable = true)\n",
      " |-- cca3: string (nullable = true)\n",
      " |-- cn: string (nullable = true)\n",
      " |-- device_id: long (nullable = true)\n",
      " |-- device_name: string (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lcd: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- scale: string (nullable = true)\n",
      " |-- temp: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4849e92-fd15-4095-a83c-752eb3e1f788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|battery_level|device_id|\n",
      "+-------------+---------+\n",
      "|            8|        1|\n",
      "|            7|        2|\n",
      "|            2|        3|\n",
      "|            6|        4|\n",
      "|            4|        5|\n",
      "|            3|        6|\n",
      "|            3|        7|\n",
      "|            0|        8|\n",
      "|            3|        9|\n",
      "|            7|       10|\n",
      "+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.select(\"battery_level\",\"device_id\")\n",
    "    .where($\"battery_level\"<10 )\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce4123dd-dce2-4e9e-b182-7195d94c20f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|              cn|             media|\n",
      "+----------------+------------------+\n",
      "|           Gabon|            1523.0|\n",
      "|Falkland Islands|            1424.0|\n",
      "|          Monaco|            1421.5|\n",
      "|          Kosovo|            1389.0|\n",
      "|      San Marino|1379.6666666666667|\n",
      "|         Liberia|            1374.5|\n",
      "|           Syria|            1345.8|\n",
      "|      Mauritania|1344.4285714285713|\n",
      "|           Congo|          1333.375|\n",
      "|           Tonga|            1323.0|\n",
      "+----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.select(\"cn\",\"c02_level\")\n",
    "    .groupBy(\"cn\")\n",
    "    .avg(\"c02_level\")\n",
    "    .withColumnRenamed(\"avg(c02_level)\",\"media\")\n",
    "    .orderBy($\"media\" desc)\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a8c2c0-83bf-495c-ad67-b8ee48da83db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da39a7-bf8e-4e2a-be67-b9bd7bdf71e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
