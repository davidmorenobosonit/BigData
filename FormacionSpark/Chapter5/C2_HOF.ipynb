{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a48da86-5b50-4866-82ab-d9719faeb937",
   "metadata": {},
   "source": [
    "# High-Order Functions\n",
    "in DataFrames and SPARK SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43620ab8-046f-499e-a051-16781df404a5",
   "metadata": {},
   "source": [
    "Normalmente necesitan el uso de funciones como:\n",
    "\n",
    "- `get_json_object()`\n",
    "- `from_json()`\n",
    "- `to_json()`\n",
    "- `explode()`\n",
    "- `selectExpr()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a0ca5-61b9-4f7b-a522-2f760b63ab84",
   "metadata": {},
   "source": [
    "### Explode and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78373b5-3095-4de6-ad7e-18a5c921546a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@f7b6ea9\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e33b2183-787c-45ff-881a-a4ee9c2906e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,9).createOrReplaceTempView(\"prueba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19888977-9b10-447f-8f7b-5eab555b826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id from prueba\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd723991-453b-43b3-aff5-85210b02cfb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " cannot resolve 'as' given input columns: [prueba.id]; line 1 pos 13;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: cannot resolve 'as' given input columns: [prueba.id]; line 1 pos 13;",
      "'Project [id#8L, 'as AS values#16]",
      "+- SubqueryAlias prueba",
      "   +- View (`prueba`, [id#8L])",
      "      +- Range (1, 9, step=1, splits=Some(8))",
      "",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:179)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:175)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:535)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:535)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:532)",
      "  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)",
      "  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)",
      "  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:467)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:532)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:181)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:209)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.immutable.List.foreach(List.scala:431)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.immutable.List.map(List.scala:305)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:209)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:214)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:323)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:214)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:181)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:161)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:175)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select id,  as values\n",
    "              from prueba \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e326bd7-934a-46c3-8435-5a34fa022b6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "16: error: ')' expected but '.' found.",
     "output_type": "error",
     "traceback": [
      "<console>:16: error: ')' expected but '.' found.",
      "         df.printSchema()",
      "           ^",
      "<console>:17: error: ';' expected but '.' found.",
      "         df.show(false)",
      "           ^",
      ""
     ]
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "val pruebaDF = Seq(\n",
    "    Row(\"James\", \"Java\"), Row(\"James\", \"C#\"),Row(\"James\", \"Python\"),\n",
    "    Row(\"Michael\", \"Java\"),Row(\"Michael\", \"PHP\"),Row(\"Michael\", \"PHP\"),\n",
    "    Row(\"Robert\", \"Java\"),Row(\"Robert\", \"Java\"),Row(\"Robert\", \"Java\"),\n",
    "    Row(\"Washington\", null)\n",
    "  )\n",
    "val arrayStructSchema = new StructType().add(\"name\", StringType)\n",
    "    .add(\"booksInterested\", StringType)\n",
    "\n",
    "val df = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(arrayStructData),arrayStructSchema)\n",
    "  df.printSchema()\n",
    "  df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b08027e6-605d-45ef-9c12-687778aa17c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- booksInterested: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "+----------+------------------+\n",
      "|name      |booksInterested   |\n",
      "+----------+------------------+\n",
      "|James     |[Java, C#, Python]|\n",
      "|Michael   |[Java, PHP, PHP]  |\n",
      "|Robert    |[Java, Java, Java]|\n",
      "|Washington|[]                |\n",
      "+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [name: string, booksInterested: array<string>]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df.groupBy(\"name\").agg(collect_list(\"booksInterested\").as(\"booksInterested\"))\n",
    "df2.printSchema()\n",
    "df2.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b81c1813-22f8-4f8d-9ae7-4b4697aa409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- booksInterested: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "+----------+------------------+\n",
      "|name      |booksInterested   |\n",
      "+----------+------------------+\n",
      "|James     |[Java, C#, Python]|\n",
      "|Michael   |[PHP, Java]       |\n",
      "|Robert    |[Java]            |\n",
      "|Washington|[]                |\n",
      "+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [name: string, booksInterested: array<string>]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = df.groupBy(\"name\").agg(collect_set(\"booksInterested\").as(\"booksInterested\"))\n",
    "df3.printSchema()\n",
    "df3.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d2fd4fc-05f1-4273-a1c7-21eca00369f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "42: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:42: error: type mismatch;",
      " found   : String(\"booksInterested\")",
      " required: org.apache.spark.sql.Column",
      "       df3.explode(\"booksInterested\").show()",
      "                   ^",
      ""
     ]
    }
   ],
   "source": [
    "df3.explode(\"booksInterested\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5da349-ba42-4e8f-9fa5-e8c8868b4291",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "26: error: not found: value Row",
     "output_type": "error",
     "traceback": [
      "<console>:26: error: not found: value Row",
      "             Row(\"James\",List(\"Java\",\"Scala\"),Map(\"hair\"->\"black\",\"eye\"->\"brown\")),",
      "             ^",
      "<console>:27: error: not found: value Row",
      "           Row(\"Michael\",List(\"Spark\",\"Java\",null),Map(\"hair\"->\"brown\",\"eye\"->null)),",
      "           ^",
      "<console>:28: error: not found: value Row",
      "           Row(\"Robert\",List(\"CSharp\",\"\"),Map(\"hair\"->\"red\",\"eye\"->\"\")),",
      "           ^",
      "<console>:29: error: not found: value Row",
      "           Row(\"Washington\",null,null),",
      "           ^",
      "<console>:30: error: not found: value Row",
      "           Row(\"Jefferson\",List(),Map())",
      "           ^",
      "<console>:33: error: not found: type StructType",
      "           val arraySchema = new StructType()",
      "                                 ^",
      "<console>:34: error: not found: value StringType",
      "             .add(\"name\",StringType)",
      "                         ^",
      "<console>:35: error: not found: value ArrayType",
      "             .add(\"knownLanguages\", ArrayType(StringType))",
      "                                    ^",
      "<console>:35: error: not found: value StringType",
      "             .add(\"knownLanguages\", ArrayType(StringType))",
      "                                              ^",
      "<console>:36: error: not found: value MapType",
      "             .add(\"properties\", MapType(StringType,StringType))",
      "                                ^",
      "<console>:36: error: not found: value StringType",
      "             .add(\"properties\", MapType(StringType,StringType))",
      "                                        ^",
      "<console>:36: error: not found: value StringType",
      "             .add(\"properties\", MapType(StringType,StringType))",
      "                                                   ^",
      ""
     ]
    }
   ],
   "source": [
    "    import spark.implicits._\n",
    "\n",
    "    val arrayData = Seq(\n",
    "      Row(\"James\",List(\"Java\",\"Scala\"),Map(\"hair\"->\"black\",\"eye\"->\"brown\")),\n",
    "    Row(\"Michael\",List(\"Spark\",\"Java\",null),Map(\"hair\"->\"brown\",\"eye\"->null)),\n",
    "    Row(\"Robert\",List(\"CSharp\",\"\"),Map(\"hair\"->\"red\",\"eye\"->\"\")),\n",
    "    Row(\"Washington\",null,null),\n",
    "    Row(\"Jefferson\",List(),Map())\n",
    "    )\n",
    "\n",
    "    val arraySchema = new StructType()\n",
    "      .add(\"name\",StringType)\n",
    "      .add(\"knownLanguages\", ArrayType(StringType))\n",
    "      .add(\"properties\", MapType(StringType,StringType))\n",
    "\n",
    "    val df = spark.createDataFrame(spark.sparkContext.parallelize(arrayData),arraySchema)\n",
    "    df.printSchema()\n",
    "    df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4523f941-4089-4fbe-ad4f-b8150bc181b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|name   |key |value|\n",
      "+-------+----+-----+\n",
      "|James  |hair|black|\n",
      "|James  |eye |brown|\n",
      "|Michael|hair|brown|\n",
      "|Michael|eye |null |\n",
      "|Robert |hair|red  |\n",
      "|Robert |eye |     |\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"name\",explode($\"properties\"))\n",
    "      .show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7891c494-11c3-485a-a04a-66643fbc5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|name   |col   |\n",
      "+-------+------+\n",
      "|James  |Java  |\n",
      "|James  |Scala |\n",
      "|Michael|Spark |\n",
      "|Michael|Java  |\n",
      "|Michael|null  |\n",
      "|Robert |CSharp|\n",
      "|Robert |      |\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"name\",explode($\"knownLanguages\"))\n",
    "      .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5de770-a577-4495-9ec2-d0c2538464fb",
   "metadata": {},
   "source": [
    "### Añadir el siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e083a884-8624-4fe0-8aca-a888b26d0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,8).createOrReplaceTempView(\"prueba2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b96d8e5-b0c8-4de2-adc0-566b2f17e3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |   prueba|       true|\n",
      "|         |  prueba2|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c48e941-8ea9-4245-af65-2eaee6e75aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mas1: Int => Int = $Lambda$4525/0x00000008415c4040@78b59db3\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mas1 = (s: Int) => {s+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95232f18-3652-46cf-b123-6f6cd74f5022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4525/0x00000008415c4040@78b59db3,IntegerType,List(Some(class[value[0]: int])),Some(class[value[0]: int]),Some(mas1),false,true)\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"mas1\",mas1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15bde2c9-8fb2-4af1-b385-24b9b602c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|mas1(id)|\n",
      "+---+--------+\n",
      "|  1|       2|\n",
      "|  2|       3|\n",
      "|  3|       4|\n",
      "|  4|       5|\n",
      "|  5|       6|\n",
      "|  6|       7|\n",
      "|  7|       8|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select id, mas1(id) from prueba2\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10e069-64df-416d-ba61-75c57e7b68f8",
   "metadata": {},
   "source": [
    "## Built-in functions for complex data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6333a2-806b-4e14-9290-54dfc3ba6b50",
   "metadata": {},
   "source": [
    "### Array types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11207342-3c4f-4b84-a751-6772c1e6585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|array_distinct(array(1, 2, 3, NULL, 3))|\n",
      "+---------------------------------------+\n",
      "|                        [1, 2, 3, null]|\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select array_distinct( array(1,2,3,null,3) )\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61e23d09-e6fa-4dd4-a589-57aab5326247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|array_intersect(array(1, 2, 3), array(1, 3, 5))|\n",
      "+-----------------------------------------------+\n",
      "|                                         [1, 3]|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT array_intersect( array(1, 2, 3), array(1,3,5)  )\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa0f633a-5659-427c-abe2-2033d3c428c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|array_union(array(1, 2, 2, 3), array(1, 3, 5))|\n",
      "+----------------------------------------------+\n",
      "|                                  [1, 2, 3, 5]|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" SELECT array_union(  array(1,2,2,3), array(1, 3, 5) )\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6ebc052-2518-4985-ba33-e1fe770d0fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|array_except(array(1, 2, 2, 3), array(1, 4))|\n",
      "+--------------------------------------------+\n",
      "|                                      [2, 3]|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" SELECT array_except( array(1,2,2,3), array(1,4) )\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88d8b673-c810-4895-9057-a4c515379b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|array_join(array(hello, world), ,)|\n",
      "+----------------------------------+\n",
      "|                       hello,world|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" SELECT array_join( array('hello','world'), ',')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "537cc15d-2455-4b60-837f-4fee1090a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|array_max(array(1, 20, NULL, 3))|\n",
      "+--------------------------------+\n",
      "|                              20|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT array_max( array(1, 20, null, 3))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7b1bc05-a48b-4af1-878d-cb6536f3c449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|array_min(array(1, 20, NULL, 3))|\n",
      "+--------------------------------+\n",
      "|                               1|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT array_min( array(1, 20, null, 3))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f13a8ca-0ca6-473f-8201-50369791d6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+\n",
      "|array_position(array(3, 2, 1, 2, 3, 1, 2, 31, 4, 1), 31)|\n",
      "+--------------------------------------------------------+\n",
      "|                                                       8|\n",
      "+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT array_position(array(3,2,1,2,3,1,2,31,4,1), 31)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1bc1f3ab-fe85-4c5d-bdfc-9967e8d212e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|array_remove(array(1, 2, 3, NULL, 3), 3)|\n",
      "+----------------------------------------+\n",
      "|                            [1, 2, null]|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT array_remove(array(1,2, 3, null, 3), 3)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5b4eb6b-3f89-4ac4-bf41-38836d059bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|arrays_overlap(array(1, 2, 3), array(3, 4, 5))|\n",
      "+----------------------------------------------+\n",
      "|                                          true|\n",
      "+----------------------------------------------+\n",
      "\n",
      "+----------------------------------------------+\n",
      "|arrays_overlap(array(1, 2, 3, 3), array(4, 5))|\n",
      "+----------------------------------------------+\n",
      "|                                         false|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT arrays_overlap( array(1,2,3), array(3, 4, 5) )\").show()\n",
    "spark.sql(\"SELECT arrays_overlap( array(1,2,3,3), array( 4, 5) )\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5935030e-e56e-414f-9a61-b6f4bc8ba306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|array_sort(array(b, d, 2, 3, NULL, c, a), lambdafunction((IF(((namedlambdavariable() IS NULL) AND (namedlambdavariable() IS NULL)), 0, (IF((namedlambdavariable() IS NULL), 1, (IF((namedlambdavariable() IS NULL), -1, (IF((namedlambdavariable() < namedlambdavariable()), -1, (IF((namedlambdavariable() > namedlambdavariable()), 1, 0)))))))))), namedlambdavariable(), namedlambdavariable()))|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[2, 3, a, b, c, d, null]                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT array_sort(array('b','d',2,3, null, 'c', 'a'))\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "715167c5-91e4-4ea6-b90d-e2bf2b20fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|concat(array(1, 2, 3), array(4, 5), array(6))|\n",
      "+---------------------------------------------+\n",
      "|                           [1, 2, 3, 4, 5, 6]|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT concat(array(1, 2, 3), array(4, 5), array(6))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b152d7c0-4d71-47bc-acb4-5aadcc292235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|flatten(array(array(1, 2, 3), array(3, 4)))|\n",
      "+-------------------------------------------+\n",
      "|                            [1, 2, 3, 3, 4]|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT flatten(  array( array(1,2,3), array(3, 4)))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a20a6b3e-11dd-4fd2-bfe3-bea43dc0b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|array_repeat(123, 3)|\n",
      "+--------------------+\n",
      "|     [123, 123, 123]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT array_repeat('123', 3)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e2cb94c4-96ed-416b-b146-c62bbb10b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|reverse(array(2, 1, 4, 3))|\n",
      "+--------------------------+\n",
      "|              [3, 4, 1, 2]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT reverse(array(2, 1, 4, 3))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e8fb1e07-cd2a-47b3-95b3-ec901660e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "| sequence(1, 5)|\n",
      "+---------------+\n",
      "|[1, 2, 3, 4, 5]|\n",
      "+---------------+\n",
      "\n",
      "+---------------+\n",
      "| sequence(5, 1)|\n",
      "+---------------+\n",
      "|[5, 4, 3, 2, 1]|\n",
      "+---------------+\n",
      "\n",
      "+----------------------------------------------------------------------+\n",
      "|sequence(to_date(2018-01-01), to_date(2018-03-01), INTERVAL '1' MONTH)|\n",
      "+----------------------------------------------------------------------+\n",
      "|[2018-01-01, 2018-02-01, 2018-03-01]                                  |\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT sequence(1, 5)\"\"\").show()\n",
    "spark.sql(\"\"\"SELECT sequence(5, 1)\"\"\").show()\n",
    "spark.sql(\"\"\"SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month)\"\"\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3e1af3aa-cb00-45be-b6e9-c2b42e652ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|shuffle(array(1, 20, NULL, 3))|\n",
      "+------------------------------+\n",
      "|              [1, null, 3, 20]|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT shuffle(array(1, 20, null, 3))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9cd1ce51-8ac3-4805-b245-8fe54000f3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|slice(array(1, 2, 3, 4), -2, 2)|\n",
      "+-------------------------------+\n",
      "|                         [3, 4]|\n",
      "+-------------------------------+\n",
      "\n",
      "+----------------------------------------------+\n",
      "|slice(array(1, 2, 3, 4, 5, 6, 7, 8, 9), -2, 2)|\n",
      "+----------------------------------------------+\n",
      "|                                        [8, 9]|\n",
      "+----------------------------------------------+\n",
      "\n",
      "+---------------------------------------------+\n",
      "|slice(array(1, 2, 3, 4, 5, 6, 7, 8, 9), 4, 3)|\n",
      "+---------------------------------------------+\n",
      "|                                    [4, 5, 6]|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT slice(array(1, 2, 3, 4), -2, 2)\").show()\n",
    "spark.sql(\"SELECT slice(array(1, 2, 3, 4,5,6,7,8,9), -2, 2)\").show()\n",
    "spark.sql(\"SELECT slice(array(1, 2, 3, 4,5,6,7,8,9), 4, 3)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c9a027e0-2929-495b-af48-69c85434a967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|arrays_zip(array(1, 2), array(2, 3), array(3, 4))|\n",
      "+-------------------------------------------------+\n",
      "|[{1, 2, 3}, {2, 3, 4}]                           |\n",
      "+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT arrays_zip(array(1, 2),array(2, 3), array(3, 4))\"\"\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "46077e7c-1f10-4426-a289-97a5ff3c0de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "|element_at(array(1, 2, 3, 3, 12, 2, 23, 5, 56, 6, 34, 43, 123, 4), 7)|\n",
      "+---------------------------------------------------------------------+\n",
      "|                                                                   23|\n",
      "+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT element_at(array(1, 2, 3,3,12,2,23,5,56,6,34,43,123,4), 7)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "510c6d67-1b16-45b1-9056-dd9f8d00ca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|cardinality(array(b, d, c, a))|\n",
      "+------------------------------+\n",
      "|                             4|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT cardinality(array('b','d', 'c', 'a'))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c56fe7-95aa-4a1b-bb7f-67cf833bdbef",
   "metadata": {},
   "source": [
    "### Map functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f47ead44-b69e-4568-8b99-1a264ce507f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|map_from_arrays(array(1.0, 3.0), array(2, 4))|\n",
      "+---------------------------------------------+\n",
      "|                         {1.0 -> 2, 3.0 -> 4}|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT map_from_arrays(array(1.0,3.0), array('2', '4'))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bc8964f7-9555-47b9-8f8c-0b9095e62d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|map_from_entries(array(struct(1, a), struct(2, b)))|\n",
      "+---------------------------------------------------+\n",
      "|                                   {1 -> a, 2 -> b}|\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT map_from_entries(array(struct(1,'a'), struct(2, 'b')))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2ec319a7-3ffa-4c96-a9f8-be5348725ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|map_concat(map(1, a, 2, b), map(4, c, 3, d))|\n",
      "+--------------------------------------------+\n",
      "|{1 -> a, 2 -> b, 4 -> c, 3 -> d}            |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT map_concat( map(1, 'a', 2, 'b'), map(4, 'c', 3, 'd') )\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "44fa3119-a78a-4f72-95d1-3ac40e79a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|element_at(map(1, a, 2, b), 2)|\n",
      "+------------------------------+\n",
      "|                             b|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT element_at(map(1, 'a', 2, 'b'), 2)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "17c47026-13fb-4295-9d81-7a86dfae1178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "|cardinality(map_concat(map(1, a, 2, b), map(3, c, 4, c)))|\n",
      "+---------------------------------------------------------+\n",
      "|                                                        4|\n",
      "+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT cardinality(map_concat(map(1, 'a', 2, 'b'),map(3,'c',4,'c')))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da83aee-55aa-494d-acb8-9fbe9cbdf2d2",
   "metadata": {},
   "source": [
    "## High Order Functions HOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fcbbe-66e3-4026-93d5-120cf45f8a6a",
   "metadata": {},
   "source": [
    "`in SQL`\n",
    "\n",
    "`transform(values, value->lambda expression)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43a064-7243-4a73-8325-5eb4a4511dfb",
   "metadata": {},
   "source": [
    "Creamos un DataFrame para poner en practica la función anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad1e65c-ae57-4cd8-9960-0b4e795f04ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1: Array[Int] = Array(35, 36, 32, 30, 40, 42, 38)\n",
       "t2: Array[Int] = Array(31, 32, 34, 55, 56)\n",
       "tC: org.apache.spark.sql.DataFrame = [celsius: array<int>]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create DataFrame with two rows of two arrays (tempc1, tempc2)\n",
    "val t1 = Array(35, 36, 32, 30, 40, 42, 38)\n",
    "val t2 = Array(31, 32, 34, 55, 56)\n",
    "val tC = Seq(t1, t2).toDF(\"celsius\")\n",
    "tC.createOrReplaceTempView(\"tC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c5dbd56-30aa-4f17-be19-81d23c9b1ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|celsius                     |\n",
      "+----------------------------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|\n",
      "|[31, 32, 34, 55, 56]        |\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tC.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df9cd1-b87f-4d04-9c8c-4afc609f4ab1",
   "metadata": {},
   "source": [
    "### `transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00bf1a58-62b4-489f-9e31-5749a5605fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------+-----------------------------------+\n",
      "|celsius                     |fahrenheit                     |kelvin                             |\n",
      "+----------------------------+-------------------------------+-----------------------------------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|[95, 96, 89, 86, 104, 107, 100]|[308, 309, 305, 303, 313, 315, 311]|\n",
      "|[31, 32, 34, 55, 56]        |[87, 89, 93, 131, 132]         |[304, 305, 307, 328, 329]          |\n",
      "+----------------------------+-------------------------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select celsius,\n",
    "                     transform(celsius, t -> (t*9) div 5 + 32) as fahrenheit,\n",
    "                     transform(celsius, t -> t+273)            as kelvin\n",
    "              from tC\n",
    "          \"\"\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05bf30-d01c-4e03-81e7-835f463d0717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc7a8423-b3e7-48d4-919f-cf73b20e3a2f",
   "metadata": {},
   "source": [
    "### `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6a16d45-6b69-4f9d-bafb-b4cffa25b5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------+\n",
      "|celsius                     |altas   |\n",
      "+----------------------------+--------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]        |[55, 56]|\n",
      "+----------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select celsius,\n",
    "                     filter(celsius, t -> t>38) as altas\n",
    "              from tC\n",
    "          \"\"\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db8a7fc-381d-4ae5-9204-067fc9219a22",
   "metadata": {},
   "source": [
    "### `exists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b8161aa-c9c1-4dab-8ea3-368eb598f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------+\n",
      "|celsius                     |prueba38|\n",
      "+----------------------------+--------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|true    |\n",
      "|[31, 32, 34, 55, 56]        |false   |\n",
      "+----------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select celsius,\n",
    "                   exists(celsius, t -> t=38) as prueba38\n",
    "             from tC\"\"\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a55b2b-2c40-49ad-9e42-c7618882602b",
   "metadata": {},
   "source": [
    "### `reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c7301e1-bac9-427a-95a5-da60822152c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 21",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 21",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$25.$anonfun$applyOrElse$111(Analyzer.scala:2101)",
      "  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:60)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$25.applyOrElse(Analyzer.scala:2101)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$25.applyOrElse(Analyzer.scala:2091)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:486)",
      "  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)",
      "  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)",
      "  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:467)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:152)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:209)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.immutable.List.foreach(List.scala:431)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.immutable.List.map(List.scala:305)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:209)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:214)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:323)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:214)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:152)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:123)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:245)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:244)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:244)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:242)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2091)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2088)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)",
      "  at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)",
      "  at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)",
      "  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)",
      "  at scala.collection.immutable.List.foreach(List.scala:431)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)",
      "  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select celsius,\n",
    "                     reduce(celsius,\n",
    "                            0,\n",
    "                            (t,acc) -> t + acc ,\n",
    "                            acc     -> (acc div size(celsius) * 9 div 5) + 32  \n",
    "                            ) as avgFah\n",
    "              from tC\n",
    "          \"\"\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54b037-2cf2-433d-bf43-bdd1dd59597f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
